
[back to homepage](https://viridyu.github.io/)

## Fast Image Processing with Fully-Convolutional Networks

### abstract
We present an approach to accelerating a wide variety of image processing operators. Our approach uses a fully-convolutional network that is trained on input-output pairs that demonstrate the operator's action. After training, the original operator need not be run at all. The trained network operates at full resolution and runs in constant time. We investigate the effect of network architecture on approximation accuracy, runtime, and memory footprint, and identify a specific architecture that balances these considerations. We evaluate the presented approach on ten advanced image processing operators, including multiple variational models, multiscale tone and detail manipulation, photographic style transfer, nonlocal dehazing, and nonphotorealistic stylization. **All operators are approximated by the same model**. Experiments demonstrate that the presented approach is significantly more accurate than prior approximation schemes. It increases approximation accuracy as measured by PSNR across the evaluated operators by 8.5 dB on the MIT-Adobe dataset (from 27.5 to 36 dB) and reduces DSSIM by a multiplicative factor of 3 compared to the most accurate prior approximation scheme, while being the fastest. We show that our models generalize across datasets and across resolutions, and investigate a number of extensions of the presented approach. The results are shown in the supplementary video at this https URL


### comments
> 1. It mentioned that, the general approach to accelerate image processing operators is to downsample the image, evaluate the operator at low resolution, and upsample.
> 2. It says to use regression loss to produce continuous prediction rather than a discrete label per pixel.
> 3. It says the advesarial training cannot improve the performance by the proposed model.
> 4. the main contribution is "one network to represent them all".I can try it in medical segmentation to see its performance.
> 5. **the adaptive batch normalization may be effective in my model.**
